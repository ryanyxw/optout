{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effd3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71076e4c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Shard is \n",
    "shard = 1\n",
    "shard_size = 1000\n",
    "\n",
    "max_length = 2048\n",
    "device = 'cuda'\n",
    "\n",
    "# ../../johnny/optout/outputs/00_0-1000.jsonl\n",
    "fn = '../../johnny/data/val.jsonl.gz'\n",
    "out_fn = '../../johnny/optout/outputs/val_%d-%d.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8be60ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = shard * shard_size\n",
    "end = start + shard_size\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5d0dc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./outputs/val_1000-2000.jsonl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_fn % (start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a6019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddbd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_jsonl_file(filename, start, end):\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        for i, line in enumerate(f, 0):\n",
    "            if i < start:\n",
    "                continue\n",
    "            if i >= end:\n",
    "                break\n",
    "                \n",
    "            json_obj = json.loads(line.strip())\n",
    "            json_obj['index'] = i\n",
    "            yield json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a149b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(stream_jsonl_file(fn, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe514a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = open(out_fn % (start, end), 'wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb65717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ddc246270149debcd6d8fefdd9591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "for json_obj in tqdm(data, total=end-start):    \n",
    "    input_ids = tokenizer.encode(json_obj['text'],  return_tensors='pt', max_length=max_length).to('cuda')\n",
    "    \n",
    "    # Evaluate the loss of the sequence with the GPT-2 model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "    # Get the loss at each token\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "    probs = torch.nn.LogSoftmax(dim=-1)(shift_logits)\n",
    "    per_token_logprobs = probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    new_obj = {}\n",
    "    new_obj['index'] = json_obj['index']\n",
    "    new_obj['meta'] = json_obj['meta']\n",
    "    new_obj['tokens'] = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())[:max_length]\n",
    "    new_obj['per_token_logprobs'] = per_token_logprobs.tolist()\n",
    "    \n",
    "    out_file.write(json.dumps(new_obj) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47dd57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0e63cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# code to check that the token level loss reflects the perplexity\\n\\nprobs = torch.nn.Softmax(dim=-1)(shift_logits)\\nprobs = probs.cpu().numpy()[0]\\nlabels = shift_labels.cpu().numpy()[0]\\n\\np = 0.0\\nfor i, j in zip(probs, labels):\\n    p += np.log(i[j])\\nprint(np.exp(-p / len(labels)))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# code to check that the token level loss reflects the perplexity\n",
    "\n",
    "probs = torch.nn.Softmax(dim=-1)(shift_logits)\n",
    "probs = probs.cpu().numpy()[0]\n",
    "labels = shift_labels.cpu().numpy()[0]\n",
    "\n",
    "p = 0.0\n",
    "for i, j in zip(probs, labels):\n",
    "    p += np.log(i[j])\n",
    "print(np.exp(-p / len(labels)))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
